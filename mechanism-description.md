# Causal dependency tracking
Each replica uses causal dependency list. The list contains value, version, and corresponding causal metadata of the request. The list is then stored in dictionary[key]. Each version have unique value, and that unique value is generated by incrementing the previous value -- initial value is 1. e.g if causal metadata is 1,2, it generates version 3, and returns version: "3", causal-metadata:"1,2,3". To keep track of what operations are done by the replica, a list called versionlist appends version of each request. So if causal-metadata is "1,2,3", the versionlist will look like [1,2,3]. If versionlist is not equal to causal-metadata, it queues that request until all previous operations are done. Note: GET request doesn't alter versionlist. 


# Detecting a downed replica
Each replica maintains a list of sockets (called REPLICAS) that are currently online. Whenever a view operation requires the current state of the system, such as returning the current view or broadcasting a put operation, a function called getView is called. Every replica has an endpoint /ping/, which simply returns a 200 status code when called. 

To ensure that no dead replica is in the REPLICAS list, getView sends a GET request to the /ping/ endpoint of every replica in its list. If a replica fails to respond with a 200 status code within 10 seconds, it is declared dead and removed from the list. The current replica will then broadcast a DELETE request to all other replicas in its list, instructing them to remove the replica. After this operation completes, all remaining replicas in REPLICAS should be live nodes.

If a new replica is added to the system, the replica will run a function called onStart. This function will scan through each replica in its REPLICAS list and attempt to ping each replica. If a replica responds to the ping with 200 status code, the replica ceases the scan and sends a PUT request to the /key-value-store-view/ endpoint on that node. This instructs the receiving node to add the new node to its view and broadcast the addition to all known replicas in the receiving node's list. After this operation terminates, every replica should have added the new node to thier view.


# Key-to-shard mapping strategy
getShardId function uses md5 hashing, then converts it into hex. To get the range from 1-shard-count, first it takes mod shard-count of that hex and add 1 to it. This method is consistent but not uniform (which is not required for keys). We use this function when handling PUT, GET, and DELETE requests to forward the request to the correct shard.

Nodes are split into shards as evenly as possible. When the first node comes online, it is responsible for sharding all nodes in its view. Every following node that comes online will request the view that the first node generated. When a node is deleted from a view, it is also deleted from its corresponding shard. 

Our resharding system works by first creating a combined dictionary that holds every key-value pair stored in every shard. The nodes are then rebalanced. Finally, the node sends a PUT request for every key-value pair in the combined dictionary. Because of our implementation of PUT, the pair will only be sent to members of the correct shard


